<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qianli Ma</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:64%;vertical-align:middle">
              <p style="text-align:left">
                <name>Qianli Ma</name>
              </p>
              <p>I am a PhD student at the <a href="https://ps.is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a>
                and <a href="https://vlg.inf.ethz.ch/">ETH Zürich</a>, co-supervised by 
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael Black</a> and 
                <a href="https://vlg.inf.ethz.ch/people/person-detail.siyutang.html">Siyu Tang </a>.
                I am also associated to the <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>.
                Prior to this, I received my Master's degree in Optics and Photonics from Karlsruhe Institute of Technology
                and Bachelor's degree in Physics from Peking University.
              </p>
              <p>
              My research uses machine learning to solve computer vision and graphics problems, with 
              a current focus on 3D representations and deformable 3D shape modeling.
              </p>
              <p style="text-align:left">
                <a href="mailto:qma@tue.mpg.de">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ks-bHqsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/qianli_m">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/qianlim/">Github</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:44%;max-width:44%;vertical-align:middle">
              <img style="width:78%;max-width:78%" alt="profile photo" src="images/me.jpg">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/POP.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qianlim.github.io/POP">
                <font color=#1772d0>  <papertitle>The Power of Points for Modeling Humans in Clothing</papertitle></font>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://qianlim.github.io/POP">Project Page</a> /
              <!-- <a href="https://github.com/qianlim/POP">Code</a> / -->
              <a href="https://arxiv.org/abs/2109.01137">arXiv</a> /
              <a href="https://youtu.be/5M4F9zSWIEE">Video</a> /
              Code coming

              <p></p>
              <p>POP — a point-based, unified model for multiple subjects and outfits that can turn a <b>single, static</b> 3D scan into an
                animatable avatar with natural pose-dependent clothing deformations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MetaAvatar.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://neuralbodies.github.io/metavatar/">
                <papertitle>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</papertitle>
              </a>
              <br>
              
              <a href=https://taconite.github.io/>Shaofei Wang</a>, 
              <a href=https://markomih.github.io/>Mihajlovic, Marko</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=http://www.cvlibs.net/>Andreas Geiger</a>, 
              <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>Siyu Tang</a>
              <br>
              arXiv, 2021
              <br>
              <a href="https://neuralbodies.github.io/metavatar/">Project Page</a> /
              <a href="https://arxiv.org/abs/2106.11944">arXiv</a> /
              Code coming

              <p></p>
              <p>A multi-subject, articulated, neural signed distance field model for clothed humans, which can fast
                create an avatar of unseen subjects from as few as 8 monocular depth images.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SCALE.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qianlim.github.io/SCALE">
                <papertitle>SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</papertitle>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=http://www-scf.usc.edu/~saitos/>Shunsuke Saito</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://qianlim.github.io/SCALE">Project Page</a> /
              <a href="https://github.com/qianlim/SCALE">Code</a> /
              <a href="https://arxiv.org/abs/2104.07660">arXiv</a> /
              <a href="https://youtu.be/v4rWCxJJzhc">Video</a>

              <p></p>
              <p>Modeling pose-dependent shapes of clothed humans explicitly with hundreds of articulated surface elements:
                 the clothing deforms naturally even in the presence of <b>topological change</b>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/scanimate.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://scanimate.is.tue.mpg.de/">
                <papertitle>SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks</papertitle>
              </a>
              <br>
              <a href=http://www-scf.usc.edu/~saitos/>Shunsuke Saito</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral, Best Paper nominee)</strong></font>
              <br>
              <a href="https://scanimate.is.tue.mpg.de/">Project Page</a> /
              <a href="https://github.com/shunsukesaito/SCANimate">Code</a> /
              <a href="https://arxiv.org/abs/2104.03313">arXiv</a> / 
              <a href="https://youtu.be/EeNFvmNuuog">Video</a>
              <p></p>
              <p>Creating an avatar with pose-dependent clothing deformation from <b>raw scans</b> without template surface registration.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/PLACE.jpeg' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
                <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
              </a>
              <br>

              <a href=https://inf.ethz.ch/people/people-atoz/person-detail.MjQyOTY2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html/>Siwei Zhang</a>, 
              <a href=https://inf.ethz.ch/people/people-atoz/person-detail.MjcwNjU2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html/>Yan Zhang</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
              <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>Siyu Tang</a>
              <br>
              <em>3DV</em>, 2020
              <br>
              <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /
              <a href="https://github.com/sanweiliti/PLACE">Code</a> / 
              <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /
              <a href="https://youtu.be/zJ1hbtMHGrw">Video</a>         
              <p></p>
              <p>An explicit representation for 3D person-​scene contact relations that enables 
                automated synthesis of realistic humans posed naturally in a given scene.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CAPE.png' width=180; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://cape.is.tue.mpg.de/">
                <papertitle>Learning to Dress 3D People in Generative Clothing</papertitle>
              </a>
              <br>
              <strong>Qianli Ma</strong>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
              <a href=https://anuragranj.github.io/>Anurag Ranjan</a>, 
              <a href=http://morpheo.inrialpes.fr/people/pujades//>Sergi Pujades</a>, 
              <a href=http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html>Gerard Pons-Moll</a>, 
              <a href=https://vlg.inf.ethz.ch/people/person-detail.siyutang.html/>Siyu Tang</a>, 
              <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://cape.is.tue.mpg.de/">Project Page</a> /
              <a href="https://github.com/QianliM/CAPE">Code</a> / 
              <a href="https://arxiv.org/abs/1907.13615">arXiv</a> / 
              <a href="https://cape.is.tue.mpg.de/dataset">Dataset</a> /
              <a href="https://youtu.be/e4W-hPFNwDE">Full Video</a> /
              <a href="https://youtu.be/NOEA-Rtq6vM">1-min Video</a> / 
              <a href="https://cape.is.tue.mpg.de/uploads/ckeditor/attachments/211/CAPE_slides.pdf">Slides</a>
              <p></p>
              <p><i>CAPE</i> &mdash; a graph-CNN-based generative model and a large-scale dataset
                 for 3D human meshes in clothing in varied poses and garment types.</p>
            </td>
          </tr>

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    
    <br><br>
    <tr style="padding:0px">
    <td>
      <p style="text-align:right;">
      <font size="2">
      Template adapted from <a href="https://leonidk.com/"><font size="2">this awesome page</font></a>.
      </font>
      </p>
    </td>
    <tr/>

  </table>


</body>

</html>
